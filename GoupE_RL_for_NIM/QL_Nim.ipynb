{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Nim with Temporal Difference\n",
    "This Notebook contains the main code to make an agent learn how to play Nim with Temporal Difference learning. The algorithm used is Q-learning.\n",
    "To run a code-block, click inside and the click \"Ctrl\"+\"enter\".\n",
    "The output of each block is situated just after this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Initialization\n",
    "In this part we initialize the variables and function needed.\n",
    "Under \"Variables initialization\", the different parameters can be modified. \"board_ini\" is the biggest board that will be learned: it represents the dimensionality of the state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import general librairies\n",
    "from time import sleep\n",
    "import random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "# personal librairies\n",
    "from SA import SA\n",
    "from AgentQ import AgentQ\n",
    "from Opponent import Opponent\n",
    "\n",
    "# Variables initialization\n",
    "##################################################\n",
    "# Reinforcement learning\n",
    "discount = 1 # no discounting (=gamma) (Keep at 1)\n",
    "stepSize = 1 # alpha , the learning rate\n",
    "epsilon = 0.8 # for the epsilon-greedy policy\n",
    "opp_epsilon = 0. # fraction of randomness for the opponent of the learning phase\n",
    "# Nim\n",
    "board_ini = sorted([5,5,5,5]) # Biggest board for learning Nim (bigger board won't be learned)\n",
    "runMax = int(3E4) # Number of runs for the learning\n",
    "##################################################\n",
    "\n",
    "# Function initialization\n",
    "def init_board():\n",
    "    \"\"\"\n",
    "    Return a random board based on board_ini\n",
    "    \"\"\"\n",
    "    for i in range(len(board_ini)):\n",
    "        board[i] = rnd.randint(0,board_ini[i])\n",
    "    board.sort()\n",
    "    \n",
    "    if board[-1] == 0:\n",
    "        return init_board()\n",
    "    return board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Reinforcement Learning\n",
    "This is in this section that the agent will actually learn. \"runMax\" (above) contains the number of runs for the learning (an output is displayed at the bottom to help keep track of the current run).\n",
    "Nothing can be modified here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Board and agent\n",
    "board = list(board_ini)\n",
    "board_end = [0] * len(board_ini)\n",
    "agent = AgentQ(SA(board), stepSize, discount, epsilon)\n",
    "oppLearning = Opponent(SA(board), policy=\"e-optimal\", epsilon=opp_epsilon)\n",
    "oppOptimal = Opponent(SA(board), policy=\"optimal\")\n",
    "\n",
    "# Learning curves lists\n",
    "learning_win = []\n",
    "greedy_win = []\n",
    "optimalMoves = []\n",
    "optimalMoves_runNb = []\n",
    "optMoveFound_Recall = []\n",
    "optMoveFound_Precision = []\n",
    "optMoveFound_F = []\n",
    "optMoveFound_runNb = []\n",
    "\n",
    "# Actual learning\n",
    "for run in range(runMax):\n",
    "    if (run+1) % 1000 == 0:\n",
    "        clear_output()\n",
    "        print(\"run   : {0}/{1}\\n\".format(run+1, runMax))\n",
    "    \n",
    "    board = init_board()\n",
    "    \n",
    "    # randomly choose the first player\n",
    "    agentIsFirst = rnd.randint(0,1)\n",
    "    if agentIsFirst == False:\n",
    "        oppLearning.move(board)\n",
    "        if board == board_end:\n",
    "            learning_win.append(-1)\n",
    "            continue\n",
    "    \n",
    "    # move until the end of the current game\n",
    "    while True:\n",
    "        agent.move(board)\n",
    "        if board == board_end:\n",
    "            agent.winUpdate()\n",
    "            learning_win.append(1)\n",
    "            break\n",
    "        \n",
    "        oppLearning.move(board)\n",
    "        if board == board_end:\n",
    "            agent.loseUpdate()\n",
    "            learning_win.append(-1)\n",
    "            break\n",
    "            \n",
    "        agent.updateQ(board)\n",
    "    \n",
    "    ## Test the agent every 100 runs on 100 more runs\n",
    "    # Here, the agent uses a greedy policy\n",
    "    if (run+1) % 100 == 0:\n",
    "        optMovePossible = 0.\n",
    "        optMoveMade = 0.\n",
    "        wins = 0.\n",
    "        for _ in range(100):\n",
    "            \n",
    "            ## The agent should always start in a winning position\n",
    "            # or it will necessarily lose against an optimal opponent\n",
    "            board = init_board()\n",
    "            before = 0\n",
    "            for i in range(len(board)):\n",
    "                before ^= board[i]\n",
    "            while before == 0:\n",
    "                board = init_board()\n",
    "                before = 0\n",
    "                for i in range(len(board)):\n",
    "                    before ^= board[i]\n",
    "            \n",
    "            while True:\n",
    "                before = 0\n",
    "                for i in range(len(board)):\n",
    "                    before ^= board[i]\n",
    "                if before != 0:\n",
    "                    optMovePossible += 1\n",
    "                    \n",
    "                agent.greedyMove(board)\n",
    "               \n",
    "                after = 0\n",
    "                for i in range(len(board)):\n",
    "                    after ^= board[i]\n",
    "                if after == 0:\n",
    "                    optMoveMade += 1\n",
    "                \n",
    "                if board == board_end:\n",
    "                    wins += 1.\n",
    "                    break\n",
    "                \n",
    "                oppOptimal.move(board)\n",
    "                if board == board_end:\n",
    "                    wins += 0.\n",
    "                    break\n",
    "        \n",
    "        greedy_win.append(wins)\n",
    "        optimalMoves.append(optMoveMade/optMovePossible*100)\n",
    "        optimalMoves_runNb.append(run)\n",
    "        \n",
    "        ## Compute current F-Score\n",
    "        # For each possible actions see if it's optimal and check if the agent\n",
    "        # considers it as optimal or not\n",
    "        optMove_P = 0. # Positives (optimal moves)\n",
    "        optMove_TP = 0. # True-positives (optimal moves seen as optimal)\n",
    "        optMove_FP = 0. # False-positives (non-optimal moves seen as optimal)\n",
    "        for s in agent.states:\n",
    "            board = list(agent.states[s])\n",
    "            for heap in range(len(board)):\n",
    "                for action in range(1,1+board[heap]):\n",
    "                    temp_board = list(board)\n",
    "                    temp_board[heap] -= action\n",
    "                              \n",
    "                    nimSum = 0\n",
    "                    for i in range(len(temp_board)):\n",
    "                        nimSum ^= temp_board[i]\n",
    "                    \n",
    "                    \n",
    "                    a = agent.actions.index([heap,action])\n",
    "                    if nimSum == 0:\n",
    "                        optMove_P += 1.\n",
    "                        if agent.Q[s][a] >= 0.9:\n",
    "                            optMove_TP += 1.\n",
    "                    elif agent.Q[s][a] >= 0.9:\n",
    "                        optMove_FP += 1.\n",
    "        \n",
    "        optMoveFound_Recall.append(optMove_TP/optMove_P)\n",
    "        if optMove_TP+optMove_FP == 0:\n",
    "            optMoveFound_Precision.append(0.)\n",
    "        else:\n",
    "            optMoveFound_Precision.append(optMove_TP/(optMove_TP+optMove_FP))\n",
    "        if optMoveFound_Precision[-1]+optMoveFound_Recall[-1] == 0:\n",
    "            optMoveFound_F.append(0.)\n",
    "        else:\n",
    "            optMoveFound_F.append(2*optMoveFound_Precision[-1]*optMoveFound_Recall[-1] / \\\n",
    "                              (optMoveFound_Precision[-1]+optMoveFound_Recall[-1]))\n",
    "        optMoveFound_runNb.append(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Learning curves\n",
    "In this section we plot the different curves used to evaluate the learning of the agent.\n",
    "Here again, nothing can be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Learning curve\n",
    "We plot here the basic learning curve, that is the reward that the agent is able to get through the learning runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Window averaging of the learning curve\n",
    "half_window = 500\n",
    "learning_win_ave = []\n",
    "\n",
    "for i in range(len(learning_win)):\n",
    "    startIndex = i - half_window\n",
    "    if startIndex < 0:\n",
    "        startIndex = 0\n",
    "        \n",
    "    endIndex = i + half_window + 1\n",
    "    if endIndex > len(learning_win):\n",
    "        endIndex = len(learning_win)\n",
    "    \n",
    "    learning_win_ave.append(float(sum(learning_win[startIndex:endIndex])) / (len(learning_win[startIndex:endIndex])))\n",
    "# Learning curve\n",
    "plt.plot(learning_win_ave)\n",
    "plt.title(\"Learning curve of the agent\")\n",
    "plt.xlabel(\"Run\"); plt.ylabel(\"Reward\")\n",
    "plt.axis([0, runMax, -1.05, 1.05]); plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the previous learning curve is actually not really helpful to assess the quality of the agent. Indeed, depending on the set of parameters, the exploration and optimality of the opponent might cause the agent to lose most of the time, even though it learns. That is why we will introduce the few next measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Winning rate\n",
    "The first way of estimating the agent is to look at its percentage of win against an optimal opponent. To do so, we won't use the learning runs because the agent loses most of the times. Instead, every 100 runs in the learning we perform another 100 runs, but this time the agent acts greedily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Winning rate (after greedization)\n",
    "plt.plot(optimalMoves_runNb, greedy_win)\n",
    "plt.title(\"Winning rate of the agent\")\n",
    "plt.xlabel(\"Run\"); plt.ylabel(\"Games won [%]\")\n",
    "plt.axis([0, runMax, 0, 105]); plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Optimal moves rate\n",
    "We know that in order to win, the agent has to act optimally all the time (if it is not already in a losing position). Then, one mistake makes him lose the game against an optimal opponent. That means that, even though it might have done good 90% of the time, it can still lose. Therefore, another way to evaluate the agent would be to see how many times it performs optimally when it can. Using the same \"greedy runs\" as in 3.2, we plot the rate of optimal moves effectuated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimal moves rate (after greedization)\n",
    "plt.plot(optimalMoves_runNb, optimalMoves)\n",
    "plt.title(\"Optimal moves rate of the agent\")\n",
    "plt.xlabel(\"Run\"); plt.ylabel(\"Optimal move done [%]\")\n",
    "plt.axis([0, runMax, 0, 105]); plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - F-Measure\n",
    "Finally, to go even further we might want to see if the agent has been able to identify every possible optimal moves, while rejecting all the others. To do so, we can think of this as a classification problem: the agent has to classify every optimal moves from the others. We can then use the F-measure to evaluate this. As the action-values are only estimations, to see if the agent considers a move as optimal, we simply compare its Q(s,a) value to a threshold (close to the actual value of an optimal move). This will be used in the grid search as the evaluation criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# F-measure (after greedization)\n",
    "plt.plot(optMoveFound_runNb, optMoveFound_F)\n",
    "plt.title(\"F-measure of the agent\")\n",
    "plt.xlabel(\"Run\"); plt.ylabel(\"F-score\")\n",
    "plt.axis([0, runMax, 0, 1.05]); plt.grid(True)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Test of the agent after learning\n",
    "Now we will test the agent against an optimal opponent. We compute the winning and optimal move rates, and compute the final value of the F-Score. The number of games can be changed below, by modifing \"trials\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "trials = 2000 # number of trials for testing\n",
    "##################################################\n",
    "wins = 0\n",
    "winStart = 0\n",
    "optMove = 0\n",
    "optDone = 0\n",
    "# Compete the agent against the optimal opponent\n",
    "for i in range(trials):\n",
    "    board = init_board()\n",
    "        \n",
    "    agentIsFirst = rnd.randint(0,1)\n",
    "    if agentIsFirst == False:\n",
    "        oppOptimal.move(board)\n",
    "        if board == board_end:\n",
    "            continue\n",
    "    \n",
    "    # See if the agent should be able to win\n",
    "    before = 0\n",
    "    for i in range(len(board)):\n",
    "        before ^= board[i]\n",
    "    if before != 0:\n",
    "        winStart += 1\n",
    "    while True:\n",
    "        # See if an optimal move is available\n",
    "        before = 0\n",
    "        for i in range(len(board)):\n",
    "            before ^= board[i]\n",
    "        if before != 0:\n",
    "            optMove += 1\n",
    "            \n",
    "        agent.greedyMove(board)\n",
    "       \n",
    "        after = 0\n",
    "        for i in range(len(board)):\n",
    "            after ^= board[i]\n",
    "        if after == 0:\n",
    "            optDone += 1\n",
    "        \n",
    "        if board == board_end:\n",
    "            wins += 1\n",
    "            break\n",
    "        \n",
    "        oppOptimal.move(board)\n",
    "        if board == board_end:\n",
    "            break\n",
    "\n",
    "print \"Agent tested over {} games:\\n\".format(trials)\n",
    "print \"Winning rate (only considering games where the agent can win):\\n\" + \\\n",
    "       \"{}/{} = {:.2f}%\\n\".format(wins, winStart, float(wins)/float(winStart)*100)\n",
    "print \"Optimal moves rate (only considering moves where an optimal one exists):\\n\" + \\\n",
    "       \"{}/{} = {:.2f}%\\n\".format(optDone, optMove, float(optDone)/float(optMove)*100)\n",
    "\n",
    "## Compute the final F-Score\n",
    "# For each possible actions see if it's optimal and check if the agent\n",
    "# considers it as optimal or not\n",
    "optMove_P = 0. # Positives (optimal moves)\n",
    "optMove_TP = 0. # True-positives (optimal moves seen as optimal)\n",
    "optMove_FP = 0. # False-positives (non-optimal moves seen as optimal)\n",
    "for s in agent.states:\n",
    "    board = list(agent.states[s])\n",
    "    for heap in range(len(board)):\n",
    "        for action in range(1,1+board[heap]):\n",
    "            temp_board = list(board)\n",
    "            temp_board[heap] -= action\n",
    "                      \n",
    "            nimSum = 0\n",
    "            for i in range(len(temp_board)):\n",
    "                nimSum ^= temp_board[i]\n",
    "            \n",
    "            \n",
    "            a = agent.actions.index([heap,action])\n",
    "            if nimSum == 0:\n",
    "                optMove_P += 1.\n",
    "                if agent.Q[s][a] >= 0.9:\n",
    "                    optMove_TP += 1.\n",
    "            elif agent.Q[s][a] >= 0.9:\n",
    "                optMove_FP += 1.\n",
    "\n",
    "optMoveFound_Recall = optMove_TP/optMove_P\n",
    "if optMove_TP+optMove_FP == 0.:\n",
    "    optMoveFound_Precision = 0.\n",
    "else:\n",
    "    optMoveFound_Precision = optMove_TP/(optMove_TP+optMove_FP)\n",
    "if optMoveFound_Precision+optMoveFound_Recall == 0:\n",
    "    optMoveFound_F = 0.\n",
    "else:\n",
    "    optMoveFound_F = 2*optMoveFound_Precision*optMoveFound_Recall / \\\n",
    "                      (optMoveFound_Precision+optMoveFound_Recall)\n",
    "        \n",
    "print \"Recall = {:.3f}\".format(optMoveFound_Recall)\n",
    "print \"Precision = {:.3f}\".format(optMoveFound_Precision)\n",
    "print \"---\"\n",
    "print \"F-Score = {:.3f}\".format(optMoveFound_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Play against the agent\n",
    "In this last section we propose you to play against the agent that has just learned. Simply run the code, and the game is going to happen in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clear_output()\n",
    "wantToPlay = True\n",
    "\n",
    "while wantToPlay:\n",
    "    print \"--------------\\nNim - New game\\n--------------\\n\"\n",
    "    \n",
    "    print \"Let's start by defining our game:\"\n",
    "    print \"There are {} heaps,\".format(len(board_ini)) + \\\n",
    "    \" but you might have some of them empty (for instance if you want to play with less).\"\n",
    "    \n",
    "    # Create the board\n",
    "    board = []\n",
    "    for x in range(len(board_ini)):\n",
    "        num = raw_input(\"Enter number of matches on heap {}: (must be between 0 and {})\\n\".format(x+1, board_ini[x]))\n",
    "        num = int(num)\n",
    "        while num < 0 or num > 5:\n",
    "            clear_output()\n",
    "            print \"Wrong number of matches!\"\n",
    "            num = raw_input(\"Enter number of matches on heap {}: (must be between 0 and {})\\n\".format(x+1, board_ini[x]))\n",
    "            num=int(num)\n",
    "        board.append(num)\n",
    "    while len(board) < len(board_ini):\n",
    "        board.append(0)\n",
    "    \n",
    "    if board == board_end:\n",
    "        clear_output()\n",
    "        print \"The board cannot be empty! Let's restart...\"\n",
    "        sleep(1.3)\n",
    "        continue\n",
    "    \n",
    "    clear_output()\n",
    "    print \"Note that the board will be sorted after each move.\"\n",
    "    print \"Current board: {}\\n\".format(board)\n",
    "    board.sort()\n",
    "    print \"Sorted board: {}\\n\".format(board)\n",
    "    \n",
    "    userStart = raw_input(\"Do you want to start? (y/n)\\n\")\n",
    "    \n",
    "    if userStart.startswith('n') or userStart.startswith('N'):\n",
    "        clear_output()\n",
    "        print \"Current board: {}\\n\".format(board)\n",
    "        sleep(1)\n",
    "        print \"The agent moves...\"\n",
    "        sleep(1.5)\n",
    "        agent.greedyMove(board)\n",
    "        clear_output()\n",
    "        print \"Current board: {}\\n\".format(board)\n",
    "        if board == board_end:\n",
    "            print \"You lost...\"\n",
    "            wantToPlay = raw_input(\"Do you want to play again? (y/n)\")\n",
    "            if wantToPlay.startswith('y') or userStart.startswith('Y'):\n",
    "                wantToPlay = True\n",
    "            else:\n",
    "                wantToPlay = False\n",
    "            continue\n",
    "    else:\n",
    "        clear_output()\n",
    "        print \"Current board: {}\\n\".format(board)\n",
    "    \n",
    "    # Moves until the end of the game\n",
    "    while True:\n",
    "        userMove = True\n",
    "        while userMove:\n",
    "            while True:\n",
    "                try:\n",
    "                    heap, num = raw_input(\"Enter heap and number of matches you \" +\\\n",
    "                                          \"want to take separated with space ex.(1 2):  \").split()\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    print \"That was no valid number.  Try again...\"\n",
    "            heap = int(heap)-1\n",
    "            num = int(num)\n",
    "            \n",
    "            if heap < 0 or heap >= len(board):\n",
    "                print \"Wrong heap! Try again\"\n",
    "                continue\n",
    "            if num < 1 or num > board[heap]:\n",
    "                print \"Wrong number! Try again\"\n",
    "                continue\n",
    "            \n",
    "            board[heap] -= num\n",
    "            board.sort()\n",
    "            userMove = False\n",
    "        \n",
    "        clear_output()\n",
    "        if board == board_end:\n",
    "            print \"You won !\"\n",
    "            break\n",
    "        \n",
    "        print \"Current board: {}\\n\".format(board)\n",
    "        sleep(1)\n",
    "        print \"The agent moves...\"\n",
    "        sleep(1.5)\n",
    "        agent.greedyMove(board)\n",
    "        clear_output()\n",
    "        print \"Current board: {}\\n\".format(board)\n",
    "        if board == board_end:\n",
    "            print \"You lost...\"\n",
    "            break\n",
    "        \n",
    "    # Replay ?\n",
    "    wantToPlay = raw_input(\"Do you want to play again? (y/n)\")\n",
    "    if wantToPlay.startswith('y') or userStart.startswith('Y'):\n",
    "        clear_output()\n",
    "        wantToPlay = True\n",
    "    else:\n",
    "        wantToPlay = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
