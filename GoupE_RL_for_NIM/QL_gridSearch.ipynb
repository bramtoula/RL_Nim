{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search on the TD's parameters\n",
    "This Notebook contains the code needed to perform the grid search for the temporal difference learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a grid search on 3 parameters:\n",
    "    - stepSize: it is the learning rate (alpha in the Q-learning algorithm)\n",
    "    - epsilon: the percentage of exploratory moves for the epsilon-greedy policy (for the learning)\n",
    "    - opp_epsilon: the percentage of random moves done by the opponent during the learning phase (the rest are optimal)\n",
    "The first two are hyperparameters of the algorithm, while the last one is a bit more arbitrary, but can still be seen as a parameter to tune in order to improve the learning.\n",
    "The evaluation of each set of parameters is based on the F-Score obtain after learning from a fixed number of runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Initialization\n",
    "In this first part we initialize the variables and function needed.\n",
    "The range of values for the parameters, as well as the number of evaluations can be modified here, under \"Variables initialization\". However, note that the maximum range of every parameter is [0,1].\n",
    "Moreover, it is possible to change the number of run after which the learning should stop. A low number may cause the learnings to be inefficient, while one too high will cause the grid search to be slow and might even prevent from discriminating correctly the performances between sets of parameters by giving most of them enough time to achieve optimality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import general librairies\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "# personal librairies\n",
    "from SA import SA\n",
    "from AgentQ import AgentQ\n",
    "from Opponent import Opponent\n",
    "\n",
    "# Variables initialization\n",
    "##################################################\n",
    "# Reinforcement learning\n",
    "discount = 1 # no discounting (=gamma) (Keep at 1)\n",
    "stepSize = [i for i in np.linspace(0,1,11)] # alpha , the learning rate\n",
    "epsilon =  [i for i in np.linspace(0,1,11)] # for the epsilon-greedy policy\n",
    "opp_epsilon = [i for i in np.linspace(0,1,4)] # fraction of randomness for the opponent of the learning phase\n",
    "# Nim\n",
    "board_ini = sorted([5,5,5,5]) # Biggest board for learning Nim (bigger board won't be learned)\n",
    "runMax = int(1E4) # Number of runs for the learning\n",
    "##################################################\n",
    "\n",
    "# Function initialization\n",
    "def init_board():\n",
    "    \"\"\"\n",
    "    Return a random board based on board_ini\n",
    "    \"\"\"\n",
    "    for i in range(len(board_ini)):\n",
    "        board[i] = rnd.randint(0,board_ini[i])\n",
    "    board.sort()\n",
    "    \n",
    "    if board[-1] == 0:\n",
    "        return init_board()\n",
    "    return board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Grid search\n",
    "The grid search is done in this section. To keep track of the evaluations, a counter is shown in the ouput. Be careful as it might take some time (usually a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Board and agent\n",
    "board = list(board_ini)\n",
    "board_end = [0] * len(board_ini)\n",
    "optMoveFound_gridSearch = np.zeros((len(stepSize),len(epsilon),len(opp_epsilon)))\n",
    "searchNb = 0\n",
    "\n",
    "# Learn for each set of parameters\n",
    "for ii in range(len(stepSize)):\n",
    "    for jj in range(len(epsilon)):\n",
    "        for kk in range(len(opp_epsilon)):\n",
    "            searchNb += 1\n",
    "            \n",
    "            agent = AgentQ(SA(board), stepSize[ii], discount, epsilon[jj])\n",
    "            oppLearning = Opponent(SA(board), policy=\"e-optimal\", epsilon=opp_epsilon[kk])\n",
    "            \n",
    "            # the learning\n",
    "            for run in range(runMax):\n",
    "                if (run+1) % 1000 == 0:\n",
    "                    clear_output()\n",
    "                    print(\"search: {}/{}\\n\".format(searchNb, len(stepSize)*len(epsilon)*len(opp_epsilon)))\n",
    "                    print(\"run   : {}/{}\\n\\n\".format(run+1, runMax))\n",
    "                \n",
    "                board = init_board()\n",
    "                \n",
    "                agentIsFirst = rnd.randint(0,1)\n",
    "                if agentIsFirst == False:\n",
    "                    oppLearning.move(board)\n",
    "                    if board == board_end:\n",
    "                        continue\n",
    "                \n",
    "                while True:\n",
    "                    agent.move(board)\n",
    "                    if board == board_end:\n",
    "                        agent.winUpdate()\n",
    "                        break\n",
    "                    \n",
    "                    oppLearning.move(board)\n",
    "                    if board == board_end:\n",
    "                        agent.loseUpdate()\n",
    "                        break\n",
    "                        \n",
    "                    agent.updateQ(board)\n",
    "             \n",
    "            # Compute the F-score after the learning for that particular set of parameters\n",
    "            optMove_P = 0.\n",
    "            optMove_TP = 0.\n",
    "            optMove_FP = 0.\n",
    "            for s in agent.states:\n",
    "                board = list(agent.states[s])\n",
    "                for heap in range(len(board)):\n",
    "                    for action in range(1,1+board[heap]):\n",
    "                        temp_board = list(board)\n",
    "                        temp_board[heap] -= action\n",
    "                                  \n",
    "                        nimSum = 0\n",
    "                        for i in range(len(temp_board)):\n",
    "                            nimSum ^= temp_board[i]\n",
    "                        \n",
    "                        a = agent.actions.index([heap,action])\n",
    "                        if nimSum == 0:\n",
    "                            optMove_P += 1.\n",
    "                            if agent.Q[s][a] >= 0.9:\n",
    "                                optMove_TP += 1.\n",
    "                        elif agent.Q[s][a] >= 0.9:\n",
    "                            optMove_FP += 1.\n",
    "            \n",
    "            optMoveFound_Recall = optMove_TP/optMove_P\n",
    "            if optMove_TP+optMove_FP == 0.:\n",
    "                optMoveFound_Precision = 0.\n",
    "            else:\n",
    "                optMoveFound_Precision = optMove_TP/(optMove_TP+optMove_FP)\n",
    "            if optMoveFound_Precision+optMoveFound_Recall == 0:\n",
    "                optMoveFound_F = 0.\n",
    "            else:\n",
    "                optMoveFound_F = 2*optMoveFound_Precision*optMoveFound_Recall / \\\n",
    "                                  (optMoveFound_Precision+optMoveFound_Recall)\n",
    "            \n",
    "            optMoveFound_gridSearch[ii,jj,kk] = optMoveFound_F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Plot of the results\n",
    "And finally we plot the results of the grid search. As there are 3 parameters, we plot multiple 2D graphs. For each graph, the optimality of the opponent is fixed, and we plot the step size vs. the exploration term.\n",
    "After, the programs return the best set of parameters it has found among the ones it tested (if there are equality, it returns only the first seen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot multiple 2D graphs\n",
    "for i in range(len(opp_epsilon)):\n",
    "    plt.imshow(optMoveFound_gridSearch[:,:,-1-i].T, origin='lower', extent=(stepSize[0], stepSize[-1], epsilon[0], epsilon[-1]), \\\n",
    "               vmin=0., vmax=1., interpolation='none', cmap='hot')\n",
    "    cbar = plt.colorbar(); cbar.set_label(\"F-Score\")\n",
    "    plt.xlabel(\"Step size (= alpha)\"); plt.ylabel(\"Epsilon (exploration term)\")\n",
    "    plt.title(\"Opponent optimal at {:.1f}%\".format((1.-opp_epsilon[-1-i])*100.))\n",
    "    plt.show()\n",
    "\n",
    "# Look for the first best set of parameters\n",
    "index_best = np.unravel_index(np.argmax(optMoveFound_gridSearch), optMoveFound_gridSearch.shape)\n",
    "print \"The optimal parameters are found to be:\"\n",
    "print \"   - step size = {}   (= alpha, the learning rate)\".format(stepSize[index_best[0]])\n",
    "print \"   - epsilon = {}     (the exploration term during learning)\".format(epsilon[index_best[1]])\n",
    "print \"   - opp_epsilon = {} (the fraction of randomness of the opponent)\".format(opp_epsilon[index_best[2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
