{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Nim with Temporal Difference\n",
    "This Notebook contains the main code to make an agent learn how to play Nim with Temporal Difference learning. The algorithm used is Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Initialization\n",
    "In this part we initialize the variables and function needed.\n",
    "Under \"Variables initialization\", the different parameters can be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import general librairies\n",
    "from time import sleep\n",
    "import random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "# personal librairies\n",
    "from SA import SA\n",
    "from AgentQ import AgentQ\n",
    "from Opponent import Opponent\n",
    "\n",
    "# Variables initialization\n",
    "##########\n",
    "# Reinforcement learning\n",
    "discount = 1 # no discounting (=gamma)\n",
    "stepSize = 1 # alpha , the learning rate\n",
    "epsilon = 1 # for the epsilon-greedy policy\n",
    "opp_epsilon = 0. # fraction of randomness for the opponent of the learning phase\n",
    "# Nim\n",
    "board_ini = sorted([5,5,5,5]) # Biggest board for learning Nim (bigger board won't be learned)\n",
    "runMax = int(3E4) # Number of runs for the learning\n",
    "##########\n",
    "\n",
    "# Function initialization\n",
    "def init_board():\n",
    "    \"\"\"\n",
    "    Return a random board based on board_ini\n",
    "    \"\"\"\n",
    "    for i in range(len(board_ini)):\n",
    "        board[i] = rnd.randint(0,board_ini[i])\n",
    "    board.sort()\n",
    "    \n",
    "    if board[-1] == 0:\n",
    "        return init_board()\n",
    "    return board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Reinforcement Learning\n",
    "This is in this section that the agent will actually learn. \"runMax\" (above) contains the number of runs for the learning (an output is displayed at the bottom to help keep track of the current run).\n",
    "Nothing can be modified here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Board and agent\n",
    "board = list(board_ini)\n",
    "board_end = [0] * len(board_ini)\n",
    "agent = AgentQ(SA(board), stepSize, discount, epsilon)\n",
    "oppLearning = Opponent(SA(board), policy=\"e-optimal\", epsilon=opp_epsilon)\n",
    "oppOptimal = Opponent(SA(board), policy=\"optimal\")\n",
    "\n",
    "# Learning curves lists\n",
    "learning_win = []\n",
    "greedy_win = []\n",
    "optimalMoves = []\n",
    "optimalMoves_runNb = []\n",
    "optMoveFound_Recall = []\n",
    "optMoveFound_Precision = []\n",
    "optMoveFound_F = []\n",
    "optMoveFound_runNb = []\n",
    "\n",
    "# Actual learning\n",
    "for run in range(runMax):\n",
    "    if (run+1) % 1000 == 0:\n",
    "        clear_output()\n",
    "        print(\"run   : {0}/{1}\\n\".format(run+1, runMax))\n",
    "    \n",
    "    board = init_board()\n",
    "    \n",
    "    # randomly choose the first player\n",
    "    agentIsFirst = rnd.randint(0,1)\n",
    "    if agentIsFirst == False:\n",
    "        oppLearning.move(board)\n",
    "        if board == board_end:\n",
    "            learning_win.append(-1)\n",
    "            continue\n",
    "    \n",
    "    # move until the end of the current game\n",
    "    while True:\n",
    "        agent.move(board)\n",
    "        if board == board_end:\n",
    "            agent.winUpdate()\n",
    "            learning_win.append(1)\n",
    "            break\n",
    "        \n",
    "        oppLearning.move(board)\n",
    "        if board == board_end:\n",
    "            agent.loseUpdate()\n",
    "            learning_win.append(-1)\n",
    "            break\n",
    "            \n",
    "        agent.updateQ(board)\n",
    "    \n",
    "    ## Test the agent every 100 runs on 100 more runs\n",
    "    # Here, the agent uses a greedy policy\n",
    "    if (run+1) % 100 == 0:\n",
    "        optMovePossible = 0.\n",
    "        optMoveMade = 0.\n",
    "        wins = 0.\n",
    "        for _ in range(100):\n",
    "            \n",
    "            ## The agent should always start in a winning position\n",
    "            # or it will necessarily lose against an optimal opponent\n",
    "            board = init_board()\n",
    "            before = 0\n",
    "            for i in range(len(board)):\n",
    "                before ^= board[i]\n",
    "            while before == 0:\n",
    "                board = init_board()\n",
    "                before = 0\n",
    "                for i in range(len(board)):\n",
    "                    before ^= board[i]\n",
    "            \n",
    "            while True:\n",
    "                before = 0\n",
    "                for i in range(len(board)):\n",
    "                    before ^= board[i]\n",
    "                if before != 0:\n",
    "                    optMovePossible += 1\n",
    "                    \n",
    "                agent.greedyMove(board)\n",
    "               \n",
    "                after = 0\n",
    "                for i in range(len(board)):\n",
    "                    after ^= board[i]\n",
    "                if after == 0:\n",
    "                    optMoveMade += 1\n",
    "                \n",
    "                if board == board_end:\n",
    "                    wins += 1.\n",
    "                    break\n",
    "                \n",
    "                oppOptimal.move(board)\n",
    "                if board == board_end:\n",
    "                    wins += 0.\n",
    "                    break\n",
    "        \n",
    "        greedy_win.append(wins)\n",
    "        optimalMoves.append(optMoveMade/optMovePossible*100)\n",
    "        optimalMoves_runNb.append(run)\n",
    "        \n",
    "        ## Compute current F-Score\n",
    "        # For each possible actions see if it's optimal and check if the agent\n",
    "        # considers it as optimal\n",
    "        optMove_P = 0. # Positives (optimal moves)\n",
    "        optMove_TP = 0. # True-positives (optimal moves seen as optimal)\n",
    "        optMove_FP = 0. # False-positives (non-optimal moves seen as optimal)\n",
    "        for s in agent.states:\n",
    "            board = list(agent.states[s])\n",
    "            for heap in range(len(board)):\n",
    "                for action in range(1,1+board[heap]):\n",
    "                    temp_board = list(board)\n",
    "                    temp_board[heap] -= action\n",
    "                              \n",
    "                    nimSum = 0\n",
    "                    for i in range(len(temp_board)):\n",
    "                        nimSum ^= temp_board[i]\n",
    "                    \n",
    "                    \n",
    "                    a = agent.actions.index([heap,action])\n",
    "                    if nimSum == 0:\n",
    "                        optMove_P += 1.\n",
    "                        if agent.Q[s][a] >= 0.9:\n",
    "                            optMove_TP += 1.\n",
    "                    elif agent.Q[s][a] >= 0.9:\n",
    "                        optMove_FP += 1.\n",
    "        \n",
    "        optMoveFound_Recall.append(optMove_TP/optMove_P)\n",
    "        if optMove_TP+optMove_FP == 0:\n",
    "            optMoveFound_Precision.append(0.)\n",
    "        else:\n",
    "            optMoveFound_Precision.append(optMove_TP/(optMove_TP+optMove_FP))\n",
    "        if optMoveFound_Precision[-1]+optMoveFound_Recall[-1] == 0:\n",
    "            optMoveFound_F.append(0.)\n",
    "        else:\n",
    "            optMoveFound_F.append(2*optMoveFound_Precision[-1]*optMoveFound_Recall[-1] / \\\n",
    "                              (optMoveFound_Precision[-1]+optMoveFound_Recall[-1]))\n",
    "        optMoveFound_runNb.append(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Learning curves\n",
    "In this section we plot the different curves used to evaluate the learning of the agent.\n",
    "Here again, nothing can be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Learning curve\n",
    "We plot here the basic learning curve, that is the reward that the agent is able to get through the learning runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAEWCAYAAAAgpUMxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8f110b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Window averaging of the learning curve\n",
    "half_window = 500\n",
    "learning_win_ave = []\n",
    "\n",
    "for i in range(len(learning_win)):\n",
    "    startIndex = i - half_window\n",
    "    if startIndex < 0:\n",
    "        startIndex = 0\n",
    "        \n",
    "    endIndex = i + half_window + 1\n",
    "    if endIndex > len(learning_win):\n",
    "        endIndex = len(learning_win)\n",
    "    \n",
    "    learning_win_ave.append(float(sum(learning_win[startIndex:endIndex])) / (len(learning_win[startIndex:endIndex])))\n",
    "# Learning curve\n",
    "plt.plot(learning_win_ave)\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Run\"); plt.ylabel(\"Reward\")\n",
    "plt.axis([0, runMax, -1.05, 1.05]); plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the previous learning curve is actually not really helpful to assess the quality of the agent. Indeed, depending on the set of parameters, the exploration and optimality of the opponent might cause the agent to lose most of the time, even though it learns. That is why we will introduce the few next measures.\n",
    "\n",
    "### 3.2 - Winning rate\n",
    "The first way of estimating the agent is to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Test of the agent after learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rate = 982/982 = 100.00%\n",
      "\n",
      "Optimal moves rate = 2566/2566 = 100.00%\n",
      "\n",
      "Recall = 1.000\n",
      "Precision = 1.000\n",
      "F-measure = 1.000\n"
     ]
    }
   ],
   "source": [
    "trials = 2000\n",
    "wins = 0\n",
    "winStart = 0\n",
    "optMove = 0\n",
    "optDone = 0\n",
    "for i in range(trials):\n",
    "    board = init_board()\n",
    "        \n",
    "    agentIsFirst = rnd.randint(0,1)\n",
    "    if agentIsFirst == False:\n",
    "        oppOptimal.move(board)\n",
    "        if board == board_end:\n",
    "            continue\n",
    "    \n",
    "    before = 0\n",
    "    for i in range(len(board)):\n",
    "        before ^= board[i]\n",
    "    if before != 0:\n",
    "        winStart += 1\n",
    "    while True:\n",
    "        before = 0\n",
    "        for i in range(len(board)):\n",
    "            before ^= board[i]\n",
    "        if before != 0:\n",
    "            optMove += 1\n",
    "            \n",
    "        agent.greedyMove(board)\n",
    "       \n",
    "        after = 0\n",
    "        for i in range(len(board)):\n",
    "            after ^= board[i]\n",
    "        if after == 0:\n",
    "            optDone += 1\n",
    "        \n",
    "        if board == board_end:\n",
    "            wins += 1\n",
    "            break\n",
    "        \n",
    "        oppOptimal.move(board)\n",
    "        if board == board_end:\n",
    "            break\n",
    "\n",
    "print \"Win rate = {}/{} = {:.2f}%\\n\\nOptimal moves rate = {}/{} = {:.2f}%\\n\".format(wins, winStart, float(wins)/float(winStart)*100, \\\n",
    "              optDone, optMove, float(optDone)/float(optMove)*100)\n",
    "\n",
    "optMove_P = 0.\n",
    "optMove_TP = 0.\n",
    "optMove_FP = 0.\n",
    "for s in agent.states:\n",
    "    board = list(agent.states[s])\n",
    "    for heap in range(len(board)):\n",
    "        for action in range(1,1+board[heap]):\n",
    "            temp_board = list(board)\n",
    "            temp_board[heap] -= action\n",
    "                      \n",
    "            nimSum = 0\n",
    "            for i in range(len(temp_board)):\n",
    "                nimSum ^= temp_board[i]\n",
    "            \n",
    "            \n",
    "            a = agent.actions.index([heap,action])\n",
    "            if nimSum == 0:\n",
    "                optMove_P += 1.\n",
    "                if agent.Q[s][a] >= 0.9:\n",
    "                    optMove_TP += 1.\n",
    "            elif agent.Q[s][a] >= 0.9:\n",
    "                optMove_FP += 1.\n",
    "\n",
    "optMoveFound_Recall = optMove_TP/optMove_P\n",
    "if optMove_TP+optMove_FP == 0.:\n",
    "    optMoveFound_Precision = 0.\n",
    "else:\n",
    "    optMoveFound_Precision = optMove_TP/(optMove_TP+optMove_FP)\n",
    "if optMoveFound_Precision+optMoveFound_Recall == 0:\n",
    "    optMoveFound_F = 0.\n",
    "else:\n",
    "    optMoveFound_F = 2*optMoveFound_Precision*optMoveFound_Recall / \\\n",
    "                      (optMoveFound_Precision+optMoveFound_Recall)\n",
    "                \n",
    "print \"Recall = {:.3f}\".format(optMoveFound_Recall)\n",
    "print \"Precision = {:.3f}\".format(optMoveFound_Precision)\n",
    "print \"F-measure = {:.3f}\".format(optMoveFound_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Play against the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You won !\n",
      "Do you want to play again? (y/n)n\n"
     ]
    }
   ],
   "source": [
    "clear_output()\n",
    "wantToPlay = True\n",
    "\n",
    "while wantToPlay:\n",
    "    print \"Nim - New game\\n\"\n",
    "    \n",
    "    print \"Let's start by defining our game:\"\n",
    "    print \"There are {} heaps, but some of them might be empty.\".format(len(board_ini))\n",
    "    \n",
    "    board = []\n",
    "    for x in range(len(board_ini)):\n",
    "        num = raw_input(\"Enter number of matches on heap {}: (must be between 0 and {})\\n\".format(x+1, board_ini[x]))\n",
    "        num = int(num)\n",
    "        while num < 0 or num > 5:\n",
    "            clear_output()\n",
    "            print \"Wrong number of matches!\"\n",
    "            num = raw_input(\"Enter number of matches on heap {}: (must be between 0 and {})\\n\".format(x+1, board_ini[x]))\n",
    "            num=int(num)\n",
    "        board.append(num)\n",
    "    while len(board) < len(board_ini):\n",
    "        board.append(0)\n",
    "    \n",
    "    if board == board_end:\n",
    "        clear_output()\n",
    "        print \"The board cannot be empty! Let's restart...\"\n",
    "        sleep(1.3)\n",
    "        continue\n",
    "    \n",
    "    clear_output()\n",
    "    print \"Note that the board will be sorted after each move.\"\n",
    "    print \"Current board: {}\\n\".format(board)\n",
    "    board.sort()\n",
    "    print \"Sorted board: {}\\n\".format(board)\n",
    "    \n",
    "    userStart = raw_input(\"Do you want to start? (y/n)\\n\")\n",
    "    \n",
    "    if userStart.startswith('n') or userStart.startswith('N'):\n",
    "        print \"The agent moves...\"\n",
    "        sleep(1.3)\n",
    "        agent.greedyMove(board)\n",
    "        clear_output()\n",
    "        print \"Current board: {}\\n\".format(board)\n",
    "    else:\n",
    "        clear_output()\n",
    "        print \"Current board: {}\\n\".format(board)\n",
    "    \n",
    "    while True:\n",
    "        userMove = True\n",
    "        while userMove:\n",
    "            heap, num = raw_input(\"Enter heap and number of matches you want to take separated with space ex.(1 2):  \").split()\n",
    "            heap = int(heap)-1\n",
    "            num = int(num)\n",
    "            \n",
    "            if heap < 0 or heap >= len(board):\n",
    "                print \"Wrong heap! Try again\"\n",
    "                continue\n",
    "            if num < 1 or num > board[heap]:\n",
    "                print \"Wrong number! Try again\"\n",
    "                continue\n",
    "            \n",
    "            board[heap] -= num\n",
    "            board.sort()\n",
    "            userMove = False\n",
    "        \n",
    "        clear_output()\n",
    "        if board == board_end:\n",
    "            print \"You won !\"\n",
    "            break\n",
    "        \n",
    "        print \"Current board: {}\\n\".format(board)\n",
    "        print \"The agent moves...\"\n",
    "        sleep(1.3)\n",
    "        agent.greedyMove(board)\n",
    "        clear_output()\n",
    "        if board == board_end:\n",
    "            print \"You lost...\"\n",
    "            break\n",
    "        print \"Current board: {}\\n\".format(board)\n",
    "        \n",
    "    wantToPlay = raw_input(\"Do you want to play again? (y/n)\")\n",
    "    if wantToPlay.startswith('y') or userStart.startswith('Y'):\n",
    "        wantToPlay = True\n",
    "    else:\n",
    "        wantToPlay = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
